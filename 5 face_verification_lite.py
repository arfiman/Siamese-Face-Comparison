# -*- coding: utf-8 -*-
"""Face Verification-v2-lite.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10t9qlmCnqMKFlzHsIeMK1O0-QWqDNRO7
"""

import numpy as np
import tensorflow as tf
import cv2

def prepare_model(model_path):
  with open(model_path, 'rb') as f:
    model_tflite = f.read()

  # Load TFLite model and allocate tensors.
  interpreter = tf.lite.Interpreter(model_content=model_tflite)
  interpreter.allocate_tensors()

  return interpreter

def detect_and_crop_faces(image):
    # Load the Haar cascade face detection model
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

    # Convert the image to grayscale
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    # Detect faces in the image
    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

    x, y, w, h = faces[0]
    # Calculate the coordinates of the bounding box
    x1 = x
    y1 = y
    x2 = x + w
    y2 = y + h

    # Crop the image around the face, maintaining color channels
    cropped_face = image[y1:y2, x1:x2].copy()
    return cropped_face

def prepare_image(image):
  return image

def predict_image(img1_path, img2_path, threshold=1.3):
  # Load image encoder model
  interpreter = prepare_model('/content/drive/MyDrive/Siamese Face Recognition/model-siamese/tflite_model.tflite')
  input_index = interpreter.get_input_details()[0]["index"]
  output_index = interpreter.get_output_details()[0]["index"]
  # Read image from url
  img1 = cv2.imread(img1_path)
  img2 = cv2.imread(img2_path)
  img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)
  img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)

  # Crop image to focus on the face
  img1 = detect_and_crop_faces(img1)
  img2 = detect_and_crop_faces(img2)

  # Prepare the image to put into the model
  img1 = cv2.resize(img1, (224, 224))
  img2 = cv2.resize(img2, (224, 224))
  img1 = np.expand_dims(img1, axis=0).astype(np.float32)
  img2 = np.expand_dims(img2, axis=0).astype(np.float32)

  # Get the encoded value of the image
  interpreter.set_tensor(input_index, img1)
  interpreter.invoke()
  tensor1 = interpreter.get_tensor(output_index)

  interpreter.set_tensor(input_index, img1)
  interpreter.invoke()
  tensor1 = interpreter.get_tensor(output_index)

  interpreter.set_tensor(input_index, img2)
  interpreter.invoke()
  tensor2 = interpreter.get_tensor(output_index)

  # Get the distance between encoded value of two images
  distance = np.sum(np.square(tensor1-tensor2), axis=-1)
  prediction = np.where(distance<=threshold, True, False)
  print(f"distance: {distance}")
  return prediction[0]